{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd0b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc9885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make training dataset and download it if we dont have it \n",
    "dataset=MNIST(root='data/',download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bed56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60e1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make test dataset keeping train as False\n",
    "test_dataset=MNIST(root='data/',train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bcb83d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x21E321F8CD0>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838c93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the whole image of index 0\n",
    "image,label=dataset[0]\n",
    "plt.imshow(image,cmap='gray')\n",
    "print('Label: ',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665b5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOQ0lEQVR4nO3dcawV5ZnH8d8jokFAAwuSG2sWtpiowSiE6KLNhk1DRUwETKxFYlhWvY2poZjVSLp/gG6MYrZsjCZNbiOW3XQlTUAkjW5RJEurpgENq1hoQXO3UK7cEDWlamCFZ/+4w+4t3HnnembmzIHn+0luzjnznDnz5OiPmXPeOfOauwvAue+8phsA0B6EHQiCsANBEHYgCMIOBHF+OzdmZnz1D9TM3W2o5aX27GY218x+a2b7zWxFmdcCUC9rdZzdzEZI+p2kOZIOStohaZG7/yaxDnt2oGZ17Nmvl7Tf3T909+OS1kuaX+L1ANSoTNgvk3Rg0OOD2bI/Y2bdZrbTzHaW2BaAksp8QTfUocIZh+nu3iOpR+IwHmhSmT37QUmXD3r8NUmHyrUDoC5lwr5D0hVmNsXMLpD0HUmbq2kLQNVaPox39y/N7AFJv5A0QtJad3+/ss4AVKrlobeWNsZndqB2tZxUA+DsQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQLU/ZDEjS2LFjk/UxY8bk1m699dbkuhMnTkzW16xZk6wfO3YsWY+mVNjNrFfSUUknJH3p7jOraApA9arYs/+tux+p4HUA1IjP7EAQZcPukraY2dtm1j3UE8ys28x2mtnOktsCUELZw/ib3P2QmV0q6VUz2+vu2wc/wd17JPVIkpl5ye0BaFGpPbu7H8pu+yW9KOn6KpoCUL2Ww25mo81s7Kn7kr4laXdVjQGoVpnD+EmSXjSzU6/z7+7+H5V0hbaZPHlysv7II48k67NmzUrWp02b9lVbGraurq5kfdmyZbVt+2zUctjd/UNJ11bYC4AaMfQGBEHYgSAIOxAEYQeCIOxAEObevpPaOIOuHldeeWVubfny5cl1Fy9enKyPGjUqWc+GXnMdOHAgt3b06NHkuldddVWyfuRI+vdXs2fPzq3t3bs3ue7ZzN2H/I/Cnh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHguBS0h3gkksuSdZXr16drN955525taJLPZe1b9++ZP3mm2/OrY0cOTK5btFY+IQJE0rVo2HPDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBMM7eARYuXJis33vvvW3q5EwffPBBsj5nzpxkPfV79qlTp7bUE1rDnh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcvQPccccdtb12b29vsr5jx45kvWjK5tQ4epGi68KjWoV7djNba2b9ZrZ70LLxZvaqme3LbsfV2yaAsoZzGP8TSXNPW7ZC0lZ3v0LS1uwxgA5WGHZ33y7p49MWz5e0Lru/TtKCatsCULVWP7NPcvc+SXL3PjO7NO+JZtYtqbvF7QCoSO1f0Ll7j6QeiYkdgSa1OvR22My6JCm77a+uJQB1aDXsmyUtye4vkfRSNe0AqEvhYbyZvSBptqQJZnZQ0kpJT0r6mZndI+n3kuobKA7gvvvuS9a7u9NfeWzZsiW3tn///uS6/f3NHZRNmjSpsW1HVBh2d1+UU/pmxb0AqBGnywJBEHYgCMIOBEHYgSAIOxAEP3HtAIcOHUrWV61a1Z5G2mzWrFlNtxAKe3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9uCWLVuWrI8ePbq2bV9zzTWl1n/zzTeT9bfeeqvU659r2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMs58FLrroomT96quvzq2tXLkyue68efNa6umU885L7y9OnjzZ8msX/c5/6dKlyfqJEyda3va5iD07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBOHsbjBw5MlmfPn16sr5hw4ZkvaurK7f2xRdfJNctGssu+k343Llzk/WicwRSzj8//b/n7bffnqw//fTTubXjx4+31NPZrHDPbmZrzazfzHYPWrbKzP5gZruyv3JnZgCo3XAO438iaah/vv/F3a/L/l6uti0AVSsMu7tvl/RxG3oBUKMyX9A9YGbvZof54/KeZGbdZrbTzHaW2BaAkloN+48kfV3SdZL6JP0w74nu3uPuM919ZovbAlCBlsLu7ofd/YS7n5T0Y0nXV9sWgKq1FHYzGzzWs1DS7rznAugM5u7pJ5i9IGm2pAmSDktamT2+TpJL6pX0XXfvK9yYWXpjZ6kLLrggWS8ai964cWOp7T/66KO5tddffz257htvvJGsjx8/Plkvev1p06Yl63VavHhxbm3Tpk3JdY8dO1ZxN+3j7jbU8sKTatx90RCLnyvdEYC24nRZIAjCDgRB2IEgCDsQBGEHgigceqt0Y2fx0FvqZ6qPPfZYct2HH3641LZfeeWVZP3uu+/OrX366afJdSdOnJisv/xy+jdOM2bMSNZTPyV96qmnkusWDdvNnz8/WU957bXXkvXVq1cn65988knL25akXbt2lVo/JW/ojT07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBOHtmxIgRyfrjjz+eW3vooYeS63722WfJ+ooVK5L19evXJ+upMd+ZM9MXCHr22WeT9aL19+/fn6zff//9ubVt27Yl17344ouT9RtvvDFZT/3E9bbbbkuuO3r06GS9yIEDB5L1KVOmlHr9FMbZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIxtkzqfFgSXrmmWdya59//nly3e7u7mR9y5YtyfoNN9yQrC9dujS3dssttyTXHTVqVLJe9Fv9559/PlkvGm9uyqJFQ100+f/dddddpV7/wQcfTNaLzk8og3F2IDjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfZMX196xunU9dWLpvfdu3dvsl702+mpU6cm62WsWrUqWX/iiSeS9RMnTlTYDarQ8ji7mV1uZtvMbI+ZvW9m38+WjzezV81sX3Y7ruqmAVRnOIfxX0r6B3e/StJfS/qemV0taYWkre5+haSt2WMAHaow7O7e5+7vZPePStoj6TJJ8yWty562TtKCmnoEUIHzv8qTzWyypOmSfi1pkrv3SQP/IJjZpTnrdEtKnxwOoHbDDruZjZG0QdJyd/+j2ZDfAZzB3Xsk9WSv0bFf0AHnumENvZnZSA0E/afuvjFbfNjMurJ6l6T+eloEUIXCPbsN7MKfk7TH3dcMKm2WtETSk9ntS7V02CYfffRRsp4aervwwguT61577bUt9XRK0bTJ27dvz61t2rQpuW5vb2+yztDauWM4h/E3Sbpb0ntmtitb9gMNhPxnZnaPpN9LuqOWDgFUojDs7v4rSXkf0L9ZbTsA6sLpskAQhB0IgrADQRB2IAjCDgTBT1wzY8eOTdYXLFiQW5sxY0Zy3f7+9PlGa9euTdZTUzJL0vHjx5N1xMKlpIHgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZgXMM4+xAcIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRGHYzexyM9tmZnvM7H0z+362fJWZ/cHMdmV/8+pvF0CrCi9eYWZdkrrc/R0zGyvpbUkLJH1b0p/c/Z+HvTEuXgHULu/iFcOZn71PUl92/6iZ7ZF0WbXtAajbV/rMbmaTJU2X9Ots0QNm9q6ZrTWzcTnrdJvZTjPbWa5VAGUM+xp0ZjZG0n9KetzdN5rZJElHJLmkf9LAof7fF7wGh/FAzfIO44cVdjMbKennkn7h7muGqE+W9HN3n1bwOoQdqFnLF5w0M5P0nKQ9g4OefXF3ykJJu8s2CaA+w/k2/huSfinpPUkns8U/kLRI0nUaOIzvlfTd7Mu81GuxZwdqVuowviqEHagf140HgiPsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EUXjByYodkfTfgx5PyJZ1ok7trVP7kuitVVX29pd5hbb+nv2MjZvtdPeZjTWQ0Km9dWpfEr21ql29cRgPBEHYgSCaDntPw9tP6dTeOrUvid5a1ZbeGv3MDqB9mt6zA2gTwg4E0UjYzWyumf3WzPab2YomeshjZr1m9l42DXWj89Nlc+j1m9nuQcvGm9mrZrYvux1yjr2GeuuIabwT04w3+t41Pf152z+zm9kISb+TNEfSQUk7JC1y99+0tZEcZtYraaa7N34Chpn9jaQ/SfrXU1NrmdlTkj529yezfyjHufsjHdLbKn3Fabxr6i1vmvG/U4PvXZXTn7eiiT379ZL2u/uH7n5c0npJ8xvoo+O5+3ZJH5+2eL6kddn9dRr4n6XtcnrrCO7e5+7vZPePSjo1zXij712ir7ZoIuyXSTow6PFBddZ87y5pi5m9bWbdTTczhEmnptnKbi9tuJ/TFU7j3U6nTTPeMe9dK9Ofl9VE2IeamqaTxv9ucvcZkm6R9L3scBXD8yNJX9fAHIB9kn7YZDPZNOMbJC139z822ctgQ/TVlvetibAflHT5oMdfk3SogT6G5O6Hstt+SS9q4GNHJzl8agbd7La/4X7+j7sfdvcT7n5S0o/V4HuXTTO+QdJP3X1jtrjx926ovtr1vjUR9h2SrjCzKWZ2gaTvSNrcQB9nMLPR2RcnMrPRkr6lzpuKerOkJdn9JZJearCXP9Mp03jnTTOuht+7xqc/d/e2/0map4Fv5D+Q9I9N9JDT119J+q/s7/2me5P0ggYO6/5HA0dE90j6C0lbJe3Lbsd3UG//poGpvd/VQLC6GurtGxr4aPiupF3Z37ym37tEX2153zhdFgiCM+iAIAg7EARhB4Ig7EAQhB0IgrADQRB2IIj/BYUFZKWUGiwbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the whole image of index 5\n",
    "image,label=dataset[5]\n",
    "plt.imshow(image,cmap='gray')\n",
    "print('Label: ',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bac04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the PIL image format to tensor format\n",
    "dataset=MNIST(root='data/',\n",
    "              train=True,\n",
    "              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab14bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label=dataset[0]\n",
    "print(img_tensor.shape,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5798da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "#Value of the image between the height 10-15 and width 10-15\n",
    "print(img_tensor[:,10:15,10:15])\n",
    "print(torch.max(img_tensor),torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f34bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the image between the height 10-15 and width 10-15\n",
    "plt.imshow(img_tensor[0,10:15,10:15],cmap='gray')\n",
    "print(torch.max(img_tensor),torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "693c2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to split the dataset into trainng and validation sets\n",
    "def split(n,val):\n",
    "    #Determine size of validation set\n",
    "    n_val=int(val*n)\n",
    "    #Create random permutation of 0 to n-1\n",
    "    index=np.random.permutation(n)\n",
    "    #Pick first n_val indices for validation set\n",
    "    return index[n_val:],index[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8383f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and validation set\n",
    "train_indices,val_indices=split(len(dataset),val=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1afc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "#Training sampler and data loader\n",
    "train_sampler=SubsetRandomSampler(train_indices)\n",
    "train_loader=DataLoader(dataset,\n",
    "                       batch_size,\n",
    "                       sampler=train_sampler)\n",
    "\n",
    "#Validation sampler and data loader\n",
    "val_sampler=SubsetRandomSampler(val_indices)\n",
    "val_loader=DataLoader(dataset,\n",
    "                       batch_size,\n",
    "                       sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bbe936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatenning out the image tensor in a vector size of 784(28*28)\n",
    "input_size=28*28\n",
    "#output of each image is a vector of size 10\n",
    "num_classes=10\n",
    "#Logistic Regression model\n",
    "model=nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4692c9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "#checking the bias and the wieghts\n",
    "print(model.weight.shape)\n",
    "print(model.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c41b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for images,labels in train_loader:\n",
    "#     print(labels)\n",
    "#     print(images.shape)\n",
    "#     ouputs=model(images)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab97eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating own model with nn.Module so that we could reshape the output\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(input_size,num_classes)\n",
    "    def forward(self,xb):\n",
    "        xb=xb.reshape(-1,784)\n",
    "        out=self.linear(xb)\n",
    "        return out\n",
    "model=MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc48662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouputs.shape:  torch.Size([100, 10])\n",
      "Sample ouputs: \n",
      " tensor([[ 1.1469e-01, -3.4256e-02,  1.3428e-01, -1.6378e-01, -3.3241e-01,\n",
      "         -6.4652e-05,  7.3024e-02, -1.2871e-02, -8.7259e-02, -1.6835e-02],\n",
      "        [ 1.6443e-01, -6.5333e-02,  7.1672e-02, -2.5387e-01, -2.3507e-01,\n",
      "         -8.0857e-02, -4.3228e-02, -2.1537e-01,  1.7341e-01, -1.6588e-01]])\n"
     ]
    }
   ],
   "source": [
    "#Output the Ouput to find the shape and values\n",
    "for images,label in train_loader:\n",
    "    outputs=model(images)\n",
    "    break\n",
    "print('ouputs.shape: ',outputs.shape)\n",
    "print('Sample ouputs: \\n',outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "076fdbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Probs:\n",
      " tensor([[0.1149, 0.0990, 0.1172, 0.0870, 0.0735, 0.1024, 0.1102, 0.1011, 0.0939,\n",
      "         0.1007],\n",
      "        [0.1244, 0.0988, 0.1133, 0.0818, 0.0834, 0.0973, 0.1010, 0.0851, 0.1255,\n",
      "         0.0894]])\n",
      "Sum:  0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "#Apply softmax for each output row\n",
    "probs=F.softmax(outputs,dim=1)\n",
    "\n",
    "#Print the Probablitites\n",
    "print('Sample Probs:\\n',probs[:2].data)\n",
    "\n",
    "#add up the probs of an output row\n",
    "print('Sum: ',torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "830af4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 8, 2, 2, 1, 8, 1, 0, 6, 7, 6, 0, 8, 1, 2, 2, 6, 2, 6, 6, 1, 2, 6, 2,\n",
      "        0, 8, 1, 0, 6, 8, 1, 1, 8, 8, 6, 2, 8, 2, 6, 6, 6, 8, 2, 2, 2, 0, 6, 1,\n",
      "        1, 8, 8, 2, 5, 2, 1, 8, 2, 8, 6, 2, 1, 2, 2, 8, 6, 2, 2, 8, 8, 8, 2, 6,\n",
      "        0, 2, 2, 6, 2, 8, 5, 1, 6, 8, 6, 6, 2, 2, 7, 6, 2, 8, 2, 2, 6, 6, 2, 8,\n",
      "        8, 2, 2, 6])\n",
      "tensor([0.1172, 0.1255, 0.1347, 0.1404, 0.1241, 0.1210, 0.1320, 0.1162, 0.1291,\n",
      "        0.1248, 0.1388, 0.1211, 0.1417, 0.1257, 0.1394, 0.1244, 0.1441, 0.1395,\n",
      "        0.1484, 0.1305, 0.1530, 0.1240, 0.1216, 0.1252, 0.1144, 0.1354, 0.1270,\n",
      "        0.1289, 0.1346, 0.1373, 0.1361, 0.1429, 0.1226, 0.1536, 0.1230, 0.1314,\n",
      "        0.1346, 0.1350, 0.1363, 0.1388, 0.1124, 0.1230, 0.1482, 0.1314, 0.1208,\n",
      "        0.1196, 0.1266, 0.1254, 0.1243, 0.1275, 0.1309, 0.1295, 0.1222, 0.1267,\n",
      "        0.1470, 0.1240, 0.1211, 0.1674, 0.1273, 0.1321, 0.1162, 0.1354, 0.1218,\n",
      "        0.1502, 0.1322, 0.1248, 0.1313, 0.1299, 0.1268, 0.1373, 0.1294, 0.1302,\n",
      "        0.1247, 0.1247, 0.1443, 0.1496, 0.1325, 0.1296, 0.1323, 0.1579, 0.1152,\n",
      "        0.1221, 0.1484, 0.1830, 0.1262, 0.1236, 0.1219, 0.1165, 0.1175, 0.1382,\n",
      "        0.1246, 0.1216, 0.1312, 0.1197, 0.1417, 0.1137, 0.1182, 0.1281, 0.1280,\n",
      "        0.1432], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#prediction \n",
    "max_probs,preds=torch.max(probs,dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3b48554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 5, 8, 9, 4, 4, 1, 3, 3, 7, 1, 5, 5, 3, 1, 0, 6, 0, 1, 2, 1, 6, 5,\n",
       "        5, 4, 4, 8, 8, 6, 0, 2, 0, 8, 1, 3, 3, 9, 7, 7, 1, 9, 4, 1, 4, 7, 1, 0,\n",
       "        5, 6, 5, 1, 5, 2, 4, 5, 1, 6, 5, 7, 0, 4, 4, 4, 2, 7, 4, 4, 7, 5, 4, 8,\n",
       "        8, 5, 4, 0, 8, 6, 8, 7, 7, 7, 0, 7, 8, 6, 7, 0, 3, 9, 5, 1, 8, 5, 1, 7,\n",
       "        5, 3, 6, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69553b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the Accuracy\n",
    "def accuracy(l1,l2):\n",
    "    return torch.sum(l1==l2).item()/len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a3d34e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7335cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3351, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#torch.max is not differentiable so mse can't be used\n",
    "#so we do cross entropy\n",
    "loss_fn=F.cross_entropy\n",
    "loss=loss_fn(outputs,label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f71993bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "learning_rate=0.001\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94fd2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model,loss_func,xb,yb,opt=None,metric=None):\n",
    "    #Calculate loss\n",
    "    preds=model(xb)\n",
    "    loss=loss_fn(preds,yb)\n",
    "    \n",
    "    #to optimize\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    #to find the acc\n",
    "    metric_result=None\n",
    "    if metric is not None:\n",
    "        #compute the metric\n",
    "        metric_result=metric(preds,yb)\n",
    "    return loss.item(),len(xb),metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a0b12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the Validation set to check the loss the model\n",
    "def evaluate(model,loss_fn,valid_dl,metric=None):\n",
    "    with torch.no_grad():\n",
    "        #pass each batch through the model\n",
    "        results=[loss_batch(model,loss_fn,xb,yb,metric=metric)for xb,yb in valid_dl]\n",
    "        \n",
    "        #seperate losses, counts and metrics\n",
    "        losses,nums,metrics=zip(*results)\n",
    "        \n",
    "        #Total size of the dataset\n",
    "        total=np.sum(nums)\n",
    "        \n",
    "        #average loss accross batches\n",
    "        avg_loss=np.sum(np.multiply(losses,nums))/total\n",
    "        avg_metric=None\n",
    "        if metric is not None:\n",
    "            #average of metric accross batches\n",
    "            avg_metric=np.sum(np.multiply(metrics,nums))/total\n",
    "    return avg_loss,total,avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dba82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    _,preds=torch.max(outputs,dim=1)\n",
    "    return torch.sum(preds==labels).item()/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05dd86de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3436,Accuracy: 0.0669\n"
     ]
    }
   ],
   "source": [
    "#Output the Validation set accuracy and loss\n",
    "val_loss,total,val_acc=evaluate(model,loss_fn,val_loader,metric=accuracy)\n",
    "print('Loss: {:.4f},Accuracy: {:.4f}'.format(val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58b3fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model with epochs\n",
    "def fit(epochs,model,loss_fn,opt,train_dl,valid_dl,metric=None):\n",
    "    for epoch in range(epochs):\n",
    "        #Training\n",
    "        for xb,yb in train_dl:\n",
    "            loss,_,_=loss_batch(model,loss_fn,xb,yb,opt)\n",
    "        \n",
    "        #Evaluation \n",
    "        result=evaluate(model,loss_fn,valid_dl,metric)\n",
    "        val_loss,total,val_metric=result\n",
    "        \n",
    "        #print\n",
    "        if metric is None:\n",
    "            print('Epoch[{}/{}], Loss: {:.4f}'.format(epoch+1,epochs,val_loss))\n",
    "        else:\n",
    "            print('Epoch[{}/{}], Loss: {:.4f}, {}: {:.4f}'.format(epoch+1,epochs,val_loss,metric.__name__,val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c19d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine model and Optimizer\n",
    "model=MnistModel()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4248184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/5], Loss: 1.8938, accuracy: 0.6674\n",
      "Epoch[2/5], Loss: 1.5890, accuracy: 0.7632\n"
     ]
    }
   ],
   "source": [
    "fit(5,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(5,model,F.cross_entropy,optimizer,train_loader,val_loader,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b2fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
